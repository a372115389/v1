hydra:
  run:
    dir: logdir/${algo}/${env_id}/${exp_id}/${seed}

algo: "dispo_value_decoder"
seed: 0
env_id: "antmaze-medium-diverse-v2"
exp_id: "value_decoder_experiment"
logdir: ${hydra:run.dir}
gamma: 0.99
reward_min: 0
reward_max: 1
num_reward_samples: 10000

feat:
  type: "fourier"
  dim: 64

training:
  num_steps: 100000
  eval_every: 1000
  save_every: 1000
  batch_size: 1024
  num_workers: 4
  resume: False
  log_psi_video: True  # Enable value visualization
  weighted_regression: False
  
model:
  embed_dim: 128
  embed_type: "positional"
  ema_rate: 0.995
  lr: 3e-4
  wd: 1e-6
  warmup_steps: 500
  sde: "vpsde"
  beta_min: 0.1
  beta_max: 20
  num_steps: 1000
  continuous: False
  
  # New ValueDecoder configuration
  task_embedding_dim: 32  # Dimension of task embedding z
  value_hidden_dims: [512, 256, 128]  # Hidden layers for ValueDecoder
  value_dropout: 0.1  # Dropout rate for ValueDecoder training

sampling:
  predictor: "ddim"
  corrector: "none"
  n_inference_steps: 1
  eta: 0.0

planning:
  planner: "random_shooting"  # NEW: Use online world model adaptation
  guidance_coef: 0.0  # Coefficient for gradient-based guidance
  num_samples: 10
  num_elites: 1
  
  # Online World Model Adaptation Parameters
  delta_phi_lr: 1e-3  # Learning rate for Δφ optimization
  adaptation_steps: 10  # Number of gradient steps for online adaptation
  kl_coef: 1.0  # λ: KL divergence regularization coefficient
  n_adaptation_samples: 32  # Number of samples for Monte Carlo estimation

eval:
  num_episodes: 10